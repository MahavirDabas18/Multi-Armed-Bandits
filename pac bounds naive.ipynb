{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> PAC BOUNDS-NAIVE APPROACH </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(array):\n",
    "    maximum=max(array)\n",
    "    minimum=min(array)\n",
    "    for i in range(len(array)):\n",
    "        array[i]=(array[i]-minimum)/(maximum-minimum)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(k,eps,iterations,mu='random'):\n",
    "    \n",
    "    #k=number of arms\n",
    "    #eps=exploration fraction-> algorithm will explore with a probability of eps and exploit with a probability of (1-eps)\n",
    "    #iterations= number of pulls of the arms\n",
    "    #mu=an array of length k, which holds the true expected value of each arm\n",
    "    #mu=\"random\"-> the expected value is sampled from a normal distribution with standard deviation 1 (default)\n",
    "    #mu=user defined-> the user sends the true expected values of each arm\n",
    "    \n",
    "    if mu==\"random\":\n",
    "        mu=np.random.normal(0,1,k) #now mu is an array containing the true expected values of each arm\n",
    "    if len(mu)!=k: #case wwhen mu is user defined\n",
    "        print(\"The length of the entered array of true expected values does not match with the number of arms entered \\n\")\n",
    "        return\n",
    "    \n",
    "    q=np.zeros(k) #the array of the estimated expected values of each arm\n",
    "    pulls=0 #number of pulls at current iteration\n",
    "    arm_pull_number=np.zeros(k) #stores the number of times each arm has been pulled till the current iteration\n",
    "    reward=0 #the reward earned at the current iteration\n",
    "    a=0 #the arm pulled at the current iteration\n",
    "    total_reward=0 #the total rewards won till the current iteration\n",
    "    avg_reward=np.zeros(iterations) #the average reward per iteration till the current iteration\n",
    "    \n",
    "    for step in range(iterations):\n",
    "        p=np.random.rand() #randomly generates a number between 0 and 1\n",
    "        \n",
    "        if eps==0 and step==0: #eps value indicated exploitation but since the steps are also zero there is no knowledge to exploit-randomly select any arm\n",
    "            a=np.random.choice(k) #action is chosen\n",
    "        \n",
    "        elif p<eps: #case of exploration\n",
    "            a=np.random.choice(k) #action is chosen randomly from the k arms\n",
    "            \n",
    "        else: #case of exploitation\n",
    "            a=np.argmax(q) #returns the action with the highest estimated expected value at current iteration\n",
    "        \n",
    "        reward=np.random.normal(mu[a],1) #the reward is sampled from the normal distribution with mean equalling the true expected value of arm and a std-dev of 1  \n",
    "        \n",
    "        #updating counts\n",
    "        pulls+=1\n",
    "        arm_pull_number[a]+=1\n",
    "        \n",
    "        #updating the rewards\n",
    "        total_reward+=reward\n",
    "        avg_reward[step]=total_reward/pulls\n",
    "        \n",
    "        \n",
    "        #updating the estimated value of arm a pulled at the current iteration\n",
    "        q[a]=((q[a]*(arm_pull_number[a]-1))+reward)/arm_pull_number[a]\n",
    "        \n",
    "    avg_reward=normalise(avg_reward) #fits the average rewards between a scale of 0 and 1\n",
    "    \n",
    "    #returns the estimated expected values of the arms after all the iterations and\n",
    "    #the average normalised reward per iteration at each iteration\n",
    "    #the number of arm pulls of each arm\n",
    "    \n",
    "    return q,avg_reward,arm_pull_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb_1(k,iterations,mu='random'):\n",
    "    \n",
    "    #k=number of arms\n",
    "    #iterations= number of pulls of the arms\n",
    "    #mu=an array of length k, which holds the true expected value of each arm\n",
    "    #mu=\"random\"-> the expected value is sampled from a normal distribution with standard deviation 1 (default)\n",
    "    #mu=user defined-> the user sends the true expected values of each arm\n",
    "    \n",
    "    if mu==\"random\":\n",
    "        mu=np.random.normal(0,1,k) #now mu is an array containing the true expected values of each arm\n",
    "    if len(mu)!=k: #case wwhen mu is user defined\n",
    "        print(\"The length of the entered array of true expected values does not match with the number of arms entered \\n\")\n",
    "        return\n",
    "    \n",
    "    q=np.zeros(k) #the array of the estimated expected values of each arm\n",
    "    pulls=0 #number of pulls at current iteration\n",
    "    arm_pull_number=np.zeros(k) #stores the number of times each arm has been pulled till the current iteration\n",
    "    reward=0 #the reward earned at the current iteration\n",
    "    a=0 #the arm pulled at the current iteration\n",
    "    total_reward=0 #the total rewards won till the current iteration\n",
    "    avg_reward=np.zeros(iterations) #the average reward per iteration till the current iteration\n",
    "    \n",
    "    for step in range(iterations):\n",
    "        \n",
    "        if step==0: #at first step we have to play each arm atleast once\n",
    "            for a in range(k):\n",
    "                reward=reward=np.random.normal(mu[a],1) #the reward is sampled from the normal distribution with mean equalling the true expected value of arm and a std-dev of 1\n",
    "                #updating counts\n",
    "                pulls+=1\n",
    "                arm_pull_number[a]+=1\n",
    "                #updating the rewards\n",
    "                total_reward+=reward\n",
    "                avg_reward[step]=total_reward/pulls\n",
    "                #updating the estimated value of arm a pulled at the current iteration\n",
    "                q[a]=((q[a]*(arm_pull_number[a]-1))+reward)/arm_pull_number[a]\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            \n",
    "            upper_bound_arm=q+np.sqrt(np.divide(2*np.log(step),arm_pull_number))\n",
    "            a=np.argmax(upper_bound_arm) #choosoing the arm with maximum upper bound\n",
    "        \n",
    "            reward=np.random.normal(mu[a],1) #the reward is sampled from the normal distribution with mean equalling the true expected value of arm and a std-dev of 1  \n",
    "        \n",
    "            #updating counts\n",
    "            pulls+=1\n",
    "            arm_pull_number[a]+=1\n",
    "        \n",
    "            #updating the rewards\n",
    "            total_reward+=reward\n",
    "            avg_reward[step]=total_reward/pulls\n",
    "        \n",
    "            #updating the estimated value of arm a pulled at the current iteration\n",
    "            q[a]=((q[a]*(arm_pull_number[a]-1))+reward)/arm_pull_number[a]\n",
    "        \n",
    "    avg_reward=normalise(avg_reward) #fits the average rewards between a scale of 0 and 1\n",
    "    \n",
    "    #returns the estimated expected values of the arms after all the iterations and\n",
    "    #the average normalised reward per iteration at each iteration\n",
    "    #the number of arm pulls of each arm\n",
    "    \n",
    "    return q,avg_reward,arm_pull_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_decay(k,iterations,mu='random'):\n",
    "    \n",
    "    #k=number of arms\n",
    "    #iterations= number of pulls of the arms\n",
    "    #mu=an array of length k, which holds the true expected value of each arm\n",
    "    #mu=\"random\"-> the expected value is sampled from a normal distribution with standard deviation 1 (default)\n",
    "    #mu=user defined-> the user sends the true expected values of each arm\n",
    "    \n",
    "    if mu==\"random\":\n",
    "        mu=np.random.normal(0,1,k) #now mu is an array containing the true expected values of each arm\n",
    "    if len(mu)!=k: #case wwhen mu is user defined\n",
    "        print(\"The length of the entered array of true expected values does not match with the number of arms entered \\n\")\n",
    "        return\n",
    "    \n",
    "    q=np.zeros(k) #the array of the estimated expected values of each arm\n",
    "    pulls=0 #number of pulls at current iteration\n",
    "    arm_pull_number=np.zeros(k) #stores the number of times each arm has been pulled till the current iteration\n",
    "    reward=0 #the reward earned at the current iteration\n",
    "    a=0 #the arm pulled at the current iteration\n",
    "    total_reward=0 #the total rewards won till the current iteration\n",
    "    avg_reward=np.zeros(iterations) #the average reward per iteration till the current iteration\n",
    "    beta=1/k #scaling factor\n",
    "    \n",
    "    #decayed epsilon=1/(1+n*beta) ->n=steps taken till the current iteration\n",
    "    \n",
    "    for step in range(iterations):\n",
    "        p=np.random.rand() #randomly generates a number between 0 and 1\n",
    "        \n",
    "        if p<1/(1+step*beta): #case of exploration\n",
    "            a=np.random.choice(k) #action is chosen randomly from the k arms\n",
    "            \n",
    "        else: #case of exploitation\n",
    "            a=np.argmax(q) #returns the action with the highest estimated expected value at current iteration\n",
    "        \n",
    "        reward=np.random.normal(mu[a],1) #the reward is sampled from the normal distribution with mean equalling the true expected value of arm and a std-dev of 1  \n",
    "        \n",
    "        #updating counts\n",
    "        pulls+=1\n",
    "        arm_pull_number[a]+=1\n",
    "        \n",
    "        #updating the rewards\n",
    "        total_reward+=reward\n",
    "        avg_reward[step]=total_reward/pulls\n",
    "        \n",
    "        \n",
    "        #updating the estimated value of arm a pulled at the current iteration\n",
    "        q[a]=((q[a]*(arm_pull_number[a]-1))+reward)/arm_pull_number[a]\n",
    "        \n",
    "    avg_reward=normalise(avg_reward) #fits the average rewards between a scale of 0 and 1\n",
    "    \n",
    "    #returns the estimated expected values of the arms after all the iterations and\n",
    "    #the average normalised reward per iteration at each iteration\n",
    "    #the number of arm pulls of each arm\n",
    "    \n",
    "    return q,avg_reward,arm_pull_number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pac(k,eps,delta,mu='random'):\n",
    "    \n",
    "    #k=number of arms\n",
    "    #iterations= number of pulls of the arms\n",
    "    #mu=an array of length k, which holds the true expected value of each arm\n",
    "    #mu=\"random\"-> the expected value is sampled from a normal distribution with standard deviation 1 (default)\n",
    "    #mu=user defined-> the user sends the true expected values of each arm\n",
    "    #eps=the solution is an eps optimal arm (defines the aprroximity part)\n",
    "    #delta=the solution is an optimal solution with a probability of 1-delta (defines the probability part)\n",
    "    \n",
    "    if mu==\"random\":\n",
    "        mu=np.random.normal(0,1,k) #now mu is an array containing the true expected values of each arm\n",
    "    if len(mu)!=k: #case wwhen mu is user defined\n",
    "        print(\"The length of the entered array of true expected values does not match with the number of arms entered \\n\")\n",
    "        return\n",
    "    \n",
    "    q=np.zeros(k) #the array of the estimated expected values of each arm\n",
    "    arm_pull_number=np.zeros(k) #stores the number of times each arm has been pulled till the current pull\n",
    "    reward=0 #the reward earned at current pull\n",
    "    a=0 #the arm pulled \n",
    "\n",
    "    #the number of times each arm has to be sampled\n",
    "    l=round((4/(eps**2))*np.log((2*k)/delta))\n",
    "    \n",
    "    for a in tqdm(range(k)):\n",
    "        for sample in range(l):\n",
    "            reward=np.random.normal(mu[a],1) #the reward is sampled from the normal distribution with mean equalling the true expected value of arm and a std-dev of 1  \n",
    "            \n",
    "            #updating counts\n",
    "            arm_pull_number[a]+=1\n",
    "            \n",
    "            #updating the estimated value of arm a pulled at the current iteration\n",
    "            q[a]=((q[a]*(arm_pull_number[a]-1))+reward)/arm_pull_number[a]\n",
    "        \n",
    "            \n",
    "    optimal_arm=np.argmax(q)        \n",
    "    \n",
    "    #returns the epsilon optimal arm with a prob of (1-delta) \n",
    "    #returns the estimated expected values of the arms after all the pulls \n",
    "    #the number of samples drawn for each arm\n",
    "    \n",
    "    return optimal_arm,q,l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 137.63it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [15:20<00:00, 92.01s/it]\n"
     ]
    }
   ],
   "source": [
    "k=10 #number of arms\n",
    "mu=np.random.normal(0,1,k) #true expected rewards of each arm\n",
    "optimal_arm01,q01,samples_drawn01=pac(k,0.1,0.1,mu)\n",
    "optimal_arm05,q05,samples_drawn05=pac(k,0.01,0.1,mu)\n",
    "optimal_arm09,q09,samples_drawn09=pac(k,0.001,0.1,mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Quantitative Analysis - PAC BOUNDS </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the true expected rewards for each arm: \n",
      "[ 1.70834451 -0.95313593 -0.91389804 -0.290868    0.37686599  0.25138032\n",
      "  0.10793535  1.73948035  0.50503734  1.41603776]\n",
      "\n",
      "\n",
      "the index of the true optimal arm:  7\n"
     ]
    }
   ],
   "source": [
    "print(\"the true expected rewards for each arm: \")\n",
    "print(mu)\n",
    "print(\"\\n\")\n",
    "a=np.argmax(mu)\n",
    "print(\"the index of the true optimal arm: \",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Case 1: epsilon=0.1 and delta=0.1 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated expected rewards for each arm: \n",
      "[ 1.69726299 -0.91194481 -0.92310409 -0.30226664  0.35582659  0.22545138\n",
      "  0.0957777   1.7596432   0.4894812   1.42753234]\n",
      "\n",
      "\n",
      "the index of the estimated optimal arm:  7\n",
      "\n",
      "\n",
      "the difference between the true and the estimated expected reward of the optimal arm: \n",
      "0.02016284525833356\n",
      "\n",
      "\n",
      "the number of pulls/samples drawn for each of the  10  arms:  2119\n"
     ]
    }
   ],
   "source": [
    "print(\"the estimated expected rewards for each arm: \")\n",
    "print(q01)\n",
    "print(\"\\n\")\n",
    "print(\"the index of the estimated optimal arm: \",optimal_arm01)\n",
    "print(\"\\n\")\n",
    "print(\"the difference between the true and the estimated expected reward of the optimal arm: \")\n",
    "print(abs(q01[a]-mu[a]))\n",
    "print(\"\\n\")\n",
    "print(\"the number of pulls/samples drawn for each of the \",k,\" arms: \",samples_drawn01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Case 2: epsilon=0.01 and delta=0.1 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated expected rewards for each arm: \n",
      "[ 1.71177688 -0.94839804 -0.91648705 -0.29049531  0.38312312  0.25185082\n",
      "  0.11144743  1.73830878  0.50637544  1.41454997]\n",
      "\n",
      "\n",
      "the index of the estimated optimal arm:  7\n",
      "\n",
      "\n",
      "the difference between the true and the estimated expected reward of the optimal arm: \n",
      "0.001171571410780059\n",
      "\n",
      "\n",
      "the number of pulls/samples drawn for each of the  10  arms:  211933\n"
     ]
    }
   ],
   "source": [
    "print(\"the estimated expected rewards for each arm: \")\n",
    "print(q05)\n",
    "print(\"\\n\")\n",
    "print(\"the index of the estimated optimal arm: \",optimal_arm05)\n",
    "print(\"\\n\")\n",
    "print(\"the difference between the true and the estimated expected reward of the optimal arm: \")\n",
    "print(abs(q05[a]-mu[a]))\n",
    "print(\"\\n\")\n",
    "print(\"the number of pulls/samples drawn for each of the \",k,\" arms: \",samples_drawn05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Case 3: epsilon=0.001 and delta=0.1 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the estimated expected rewards for each arm: \n",
      "[ 1.70855173 -0.95298418 -0.91368171 -0.2911788   0.3767244   0.25122893\n",
      "  0.10807299  1.73954543  0.50487605  1.41608851]\n",
      "\n",
      "\n",
      "the index of the estimated optimal arm:  7\n",
      "\n",
      "\n",
      "the difference between the true and the estimated expected reward of the optimal arm: \n",
      "6.507669860456033e-05\n",
      "\n",
      "\n",
      "the number of pulls/samples drawn for each of the  10  arms:  21193269\n"
     ]
    }
   ],
   "source": [
    "print(\"the estimated expected rewards for each arm: \")\n",
    "print(q09)\n",
    "print(\"\\n\")\n",
    "print(\"the index of the estimated optimal arm: \",optimal_arm09)\n",
    "print(\"\\n\")\n",
    "print(\"the difference between the true and the estimated expected reward of the optimal arm: \")\n",
    "print(abs(q09[a]-mu[a]))\n",
    "print(\"\\n\")\n",
    "print(\"the number of pulls/samples drawn for each of the \",k,\" arms: \",samples_drawn09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Comparison is done on the basis of different values of epsilon keeping the probability of finding the optimal arm (1-delta)\n",
    "the same for all three cases\n",
    "\n",
    "-the true optimal arm was returned for all eps=0.1, 0.01 and 0.001\n",
    "\n",
    "-the number of samples drawn for each arm for the respective eps=0.1,0.01 and 0.001 are 2119, 211933 and 21193269\n",
    "\n",
    "-the estimated expected rewards were the closest to the true expected rewards for eps=0.001 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
